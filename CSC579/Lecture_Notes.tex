%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{units}
\usepackage{amsfonts}

\newcommand\TP{\mathit{TP}}
\newcommand\FN{\mathit{FN}}
\begin{document}


\title{CSC579 : Class Notes}
\section {Administrivia}
\section{Probability Review}
Put review of probability here. 
\section{ Stochastic Processes and Markov Models }
	\subsection{Stochastic Procsses}
	  A family of Random vars $\{ X(t) : t\in T\}$ 
	\\\indent t is normally time  
    \\\indent T is called the index set 
    \\\indent T can be discrete $\{ 0,1,2,3 \ldots \}$ or cont $\{ t : 0 \le t \le \infty \}$
	\\\indent the set  $\{ X(t) : t \in T \}$ is the State Space 
	\\
	\subsection{Time Dependence}
	Stationarity -- invariant under a shift of time origin. 
	\[Prob\{ X(t_1) \le t_1, X(t_2) \le t_2 \ldots, X(t_n) \le t_n \} = \]
	\[Prob\{ X(t_1 + \alpha ) \le t_1, X(t_2 + \alpha ) \le t_2 \ldots, X(t_n +\alpha) \le t_n \}\]
	Homogenerity -- invariant under time elapsed from origin
	\\
	\subsection{Markov process}
	A stochastic process who satisfies Markov or Memoryless property. 
	\[Prob\{ X_{n+1} = x_{n+1} | X_n = x_n, X_{n-1} = x_{n-1} \ldots , X_0 = x_0 \}\]
	\[= Prob\{ X_{n+1} = x_{n+1} | X_n = x_n\}\]
	\\Single step transition probability :
	\[Prob\{X_{n+1} = j | X_n = i \} = p_{ij}(n) \]
	\\Transition Probability Matrix : 
	\[ \left( \begin{array}{ccc}
            p_{00}(n) & p_{01}(n) & p_{02}(n) \\
			p_{10}(n) & p_{11}(n) & p_{12}(n) \\
			p_{20}(n) & p_{21}(n) & p_{22}(n) 
	\end{array} \right)\] 
	(Row is from, Column is to, p10 is from 1 to 0.)
 	\\$p_{ab}(n)$ is the probability to move from a to b at timestep n.
	\\
 	\\If the markov chain is homogeneous, this matrix is the same for all values of n. 
	\[sum\limits_{all j}{p_{ij}(n)} = 1\]
	For a homogeneous markov chain, $p_{ij} p_{jk} p_{kl}$ is the probability of traveling $i\rightarrow j\rightarrow k\rightarrow l$
	\subsubsection{Example : Social Modility Model }
	Partition into upper middle and lower class. 
	
	\[P = \bordermatrix{   
              & U    & M    & L     \cr
            U & 0.45 & 0.50 & 0.05 \cr
			M & 0.15 & 0.65 & 0.20 \cr
			L & 0.00 & 0.50 & 0.50  }
    \]

	$$Prob\{ X_{n+2} = U, X_{n+1} = M | X_n = L \} = P_{ML}P_{MU} = 0.5 \times 0.15 = 0.075 $$
 	\subsubsection{Example : The Ehrenfest Model}
 	Two buckets containing N balls - choose one ball at random and move to the other bucket, let k indicate the number of buckets in bucket 1. 
 	\[ Prob\{ X_{n+1} = k + 1 | X_{n} = k \} = \frac{ N - k}{N}, \quad Prob\{ X_{n+1} = k-1 | X_n \} = \frac{k}{N}, \quad k \geq 1.\] 
 	\[p_{01} = 1 \quad p_{12} = \frac{5}{6} \quad p_{23} = \frac{4}{6} \quad \ldots \quad etc. \]
 	\subsubsection{Example : A Non-homogeneous example }
 	\[p_{aa}(n) = p_{bb}(n)= 1/n ,\quad p_{ab}(n) = p_{ba}(n) = \frac{n - 1}{n}, \quad \forall n \in N \]
 	\[ P(n) = \left( \begin{array}{cc}
            \nicefrac{1}{n} & \nicefrac{(n - 1)}{n}  \\
			\nicefrac{(n - 1)}{n} & \nicefrac{1}{n}  
	\end{array} \right)\]  
	h
	As time increases the probability of switching states decreases approaching 0 and n approaches \(\infty\)
	\subsection{Making Markov out of non-memoryless stochastic processes}
	Consider the belfast weather morkov chain. If it were the case that the tranisition probaility matrix of a state where it was 2 rainy days in a row was
	different than a single rainy day, then our stochastic process is no longer memoryless. We can flatten this out by creating a new state to accomondate, called
	RR, making our states $\{S,C,R,RR\}$. This is similar to flattening out a non-deterministic state machine to a deterministic one - it could result in a
	infinite number of states - but is sometime usefull to make markov chains out of non-memoryless stochastic processes. 
	\\
	\\If the last k steps of a stochastic process are important, than stochastic process with s steps can be flattened into a markov chain with $s^k$ states.
	\\
	\\When a stochastic process is dependent on k previous steps it is called a k-dependent process.  
	\subsection{The Embedded Markov Chain (Analysis of propensity to move, and normalizing with no self loops)}
	Diagonal elements of a TPM can be non-zero, Soujour time is the amount of time spent in a state before moving out. 
	\\For a homogenous soujourn time, the soujour time must be geometric.
	\\
	\\Geometric random variable : 
	\[ \alpha > 0,\quad \alpha(1-\alpha) ^ { n -1 }, \quad n \in \mathbb{N} \]
	\\Similar to tossing a coin weighted coin until you get heads, stay in state until you get heads.
	\\
	\\Prbability of leaving a state i :
	\[ \sum\limits_{i\neq j}{ p_{ij}} = (1 - p_{ii}) \]
	\\Probability that we spend k steps in state i:
	\[ Prob\{R_i=k\} = (1 - p_{ii})p_{ii}^{k-1}\]
	\[ E[R_i] = \frac{1}{1-p_{ii}}, \quad Var[R_i] =\frac{p_{ii}}{(1-p_{ii})^2} \]
	\\We can normalize a markov chain to remove self loops $\Longrightarrow$ replace $p_{ij}, i \neq j$ with $p_{ij} / (1 - p_{ii} )$, set diagonal to zeros.
	\\The new matrix is still stochastic :
	\[\sum\limits_{i\neq j}{\frac{p_{ij}}{1 - p_{ii}}} = \frac{1}{1-p_{ii}}, \quad\sum\limits_{i\neq j}{p_{ij}} =\frac{1}{1-p_{ii}} ( 1-p_{ii} )= 1\]
	\\Embedded markov chains, or normalized markov chains lose information.  Recoding the Sojourn times for each nodes allows us to go backwards and know
	something about the self loops in an embedded markov chain. 
	\subsection {Chapman-Kolmogorov Equation}
	Belfast process with states $\{R,S,C\}$. 
	\\Probability that its couldy two days from now given today is sunny: 
	\[ \sum\limits_{w=R,C,S}{p_{Sw}p_{wC}}. \]
	\\Find this sum by computing the dot product of the row corresponding to all states leaving the initial state, and the column corresponding to all state
	entering the destination state. 
	\[P = \bordermatrix{   
              & S    & C    & R    \cr
            S & a & d & g \cr
			C & b & e & h \cr
			R & c & f & i } 
			, \quad V_{Sw} = <a,d,g>
			, \quad V_{wC} = <d,e,f>
			, \quad \sum\limits_{w=R,C,S}{p_{Sw}p_{wC}} = V_{Sw}V_{wC}
    \]
	\\The cross product of the matrix is the dot product of all of these elements so $P^k$ is the probability of arriving at a state k steps down
	\[P^2= \bordermatrix{   
              & S    & C    & R    \cr
            S & V_{Sw}V_{wS} & V_{Sw}V_{wC} & V_{Sw}V_{wR} \cr
			C & V_{Cw}V_{wS} & V_{Cw}V_{wC} & V_{Cw}V_{wR} \cr
			R & V_{Rw}V_{wS} & V_{Rw}V_{wC} & V_{Rw}V_{wR} }  
	 ,\quad P^3= \bordermatrix{   
              & S    & C    & R    \cr
            S & V_{Sw}V_{wx}V_{xS} & V_{Sw}V_{wx}V_{xC} & V_{Sw}V_{wx}V_{xR} \cr
			C & V_{Cw}V_{wx}V_{xS} & V_{Cw}V_{wx}V_{xC} & V_{Cw}V_{wx}V_{xR} \cr
			R & V_{Rw}V_{wx}V_{xS} & V_{Rw}V_{wx}V_{xC} & V_{Rw}V_{wx}V_{xR} }  
	,\quad etc\ldots \]
	\\Limit as k appraoches infinity is the total probability to be in a state after a long time.  
	\\Generalize to compute probablities after discrete time steps 
	\[ p_{ij}^{m} = Prob\{ X_{n+m} = j | X_{n} = i \} \]
	\\Can be obtain from single step probabilities (Without raising matrixes to powers)
	\[ p_{ij}^{(m)} = \sum\limits_{\forall k}{p_{ik}^{(l)}p_{kj}^{(m-l)}}, 0 < l < m. \rightarrow P^{(m)} = P^{(l)}P^{(m-l)}\]
	\\This is the Chapman - Kolmogorov equation.
	\\Non homogeneous matrixes must accomodate n. 
    \subsection{Postion Probability Vector ``\pi''}
	Let \(\pi_i^{(k)}\) be the probability that a markov chain is in poition i at time step k. \(k = 0\) is the initial step. 
	\\\indent\(\pi^{(k)}\) is the column vector of these probabilites. 
    \\\indent\(\pi^{(k+1)} = \pi^{(k)}P(k)\) or in a homogenous Markov chain \(\pi^{(k+1)} = \pi^{(k)}P\)
    \\
    \\Example:
    \[\pi^{(k+1)} = \pi^{(k)}P(k) = \left(0,.5,.5\right) \left( \begin{array}{ccc}
			0.8&0.15&0.05  \\
			0.7&0.2&0.1  \\
			0.5&0.3&0.2   
	\end{array} \right) = \left(0.6, 0.25, 0.15\right).\]
	\\
	\\In a general for a homogenous or a non-homogenous example :
	\[\left(\pi^{m} =\pi^{(0)}P^{(m)} \right)\mbox{ or }\left( \pi^{m} = \pi^{(0)}\prod\limits_{i=0}^{m}{P(i)} \right)\mbox{ in a non-homogenous example}\]
	\\
	\\It is useful to look at the limit as timestep approached infinity when this is available.
	\[ \lim_{n\rightarrow\infty}{\pi^{(n)}} = \pi^{(0)}\lim_{n\rightarrow\infty}{P^{(n)}}\]
	\\
	\\ Example : Kafkaesque prison. A prisoner must flip 6 coins. If he ever flips 3 consecutive heads, he is executed. If he flips 6 without getting 3 consecutive
	heads he lives. 
	\\
	\\ Create 4 states $H_0, H_1, H_2, H_3\}$, where the subscript refers to the consectutive number of heads he has seen 
	\\Use the following tranistion probability matrix (Note this example is homogenous.)
	\[P= \bordermatrix{   
                & H_0 & H_1 & H_2  & H_3 \cr
            H_0 & 0.5 & 0.5 & 0    & 0   \cr
            H_1 & 0.5 & 0   & 0.5  & 0   \cr
            H_2 & 0.5 & 0   & 0    & 0.5 \cr
            H_3 & 0   & 0   & 0.5  & 1
       },\quad \pi_0 = \left(1,0,0,0\right)\]
       \[P^6 = \left(\matrix{
         0.375 & 0.203 & 0.109 & 0.312 \cr
         0.312 & 0.171 & 0.093 & 0.421 \cr
         0.203 & 0.109 & 0.062 & 0.062 \cr
         0     & 0     & 0     & 1.0  
       }\right), \quad \pi_0P^6 = \left( 0.375, 0.203, 0.109, 0.312\right)\]
    Chance of death is 31.2\%.
    \section{Classification of States}
    \underline{Reccurrent States} : The DTMC is garunteed to return to these state infinitely often
    \\State 6 and 7 and 8 are recurrent
    \\\underline{Positive Recurrent} : there is a finite number of steps required to return. 
    \\\underline{Negative Recurrent} : these is an infinit number of steps required to return to the state. 
    \\\underline{Transient States} : the is a non zero probability the markov chain will never return
    \\State 3 and 4 are transient
    \\\underline{Ephemeral States} : states that only last for the first timestep, emphemeral is a advanced form of transient. 
    \\States 1 and 2 are ephmeral and transient
    \\\underline{Periodic State} : A state to which a MC return after ever so many states, with a period p.
    \\State 6 and 7 are periodic 
    \\\underline{Absorbing State} : A state that once the MC reaches it it will never leave. 
    \\State 8 is absorbing
    \[\bordermatrix{
        & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \cr
      1 & 0 & 0 & .5& .5&0  &0  &0  &0  \cr
      2 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \cr
      3 & 0 & 0 & 0 & .5& 0 & .5& 0 &0  \cr
      4 & 0 & 0 & 1& 0 & 0 & 0 & 0 & 0 \cr
      5 &0  &0  &0  &0  &.33&0  &.33&.33\cr
      6 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \cr
      7 &  0&  0&  0& 0 & 0 &  1&0  & 0 \cr
      8 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\cr    
       }\]
   
   	\subsection{Mathmatical Analysis of Recurrent States}
   	\subsubsection{Calculation of \(f_{jj}^{(n)}\)}
    $P_{jj}^{(n)}$ is the probability that a markov returns to j from j after n steps. 
   	\\$f_{jj}^{(n)}$ is the probability that a markov returns to j from j \underline{for the first time} after n steps. 
   	\\
   	\\(I)
   	\[ f_{jj}^{(1)} = p_{jj}^{(1)} \]
    (II)
    \[ p_{jj}^{(2)} = f_{jj}^{(1)}p_{jj}^{(1)} + f_{jj}^{(2)}\] 
    \[ f_{jj}^{(2)} = p_{jj}^{(2)} - p_{jj}^{(1)}f_{jj}^{(1)}\]
    (III)
    \[ p_{jj}^3 = f_{jj}^{(1)}( f_{jj}^{(1)}p_{jj}^{(1)} + f_{jj}^{(2)} ) + f_{jj}^{(2)}p_{jj}^{(1)} + f_{jj}^{(3)}p_{jj}^{(0)} \]
    \[ p_{jj}^3 = f_{jj}^{(1)}p_{jj}^{(2)} + f_{jj}^{(2)}p_{jj}^{(1)} + f_{jj}^{(3)}p_{jj}^{(0)} \]
    \[ f_{jj}^{(3)} = p_{jj}^{(3)} - f_{jj}^{(1)}p_{jj}^{(2)} - f_{jj}^{(2)}p_{jj}^{(1)}\]
 	(IV)
 	\[f_{jj}^{(n)} = p_{jj}^{(n)} - \sum\limits_{i = 1}^{n-1}{f_{jj}^{(l)}p_{jj}^{(n-l)}}, \quad n \geq 1.\]
 	\\
 	\\Probability of ever returning to j : 
 	\[f_{jj} = \sum\limits_{n=1}^{\infty}{f_{jj}^{(n)}} \]
 	If this equals 1, then state j is recurrent. 
    
    \subsubsection{Recurrence and Expected Number of Times in a State}
    State j is recurrent \(\iff\) the probability of return to j is 1 
 	\\State j is recurrent \( \Rightarrow \exists n > 0 \mbox{ s.t. } p_{jj}^{(n)} >0 \)
	\\   
    \\ We show that expected number of return to a recurrent state is infinity
    \\ Let \(I_n = 1\) if in state \(j\) at \(n\) else \(0 \)
    \\ \(\sum\nolimits_{n=0}^{\infty}{I_n}\) is number of times in state j.
    \[E\left[ \sum\limits_{n=0}^{\infty}{I_n|X_0=j}\right]=\sum\limits_{n=0}^{\infty}{E\left[I_n|X_0=j\right]} = \sum\limits_{n=0}^{\infty}{P\{X_n=j|X_0=j\}}\]
    \[= \sum\limits_{n=0}^{\infty}{p_{jj}^{(n)}} = \infty \]
    Thus when state j is recurrent we expect to visit the state an infinite number of times as time goes to infinity
	\subsection{Mathematical classification of Transient States}
	If \(f_{jj} < 1 \) then the state j is said to be transient - nonzero probability that the chain will never return. 
	\\The probability to never return to state j is \(( 1 - f_{jj})\)
	\\View this as a set bernoulli trials. Probability of returning to the state k times is \(1-f_{jj})f_{jj}^{k}\)
	\\According to the priciples of bernoulli trials, the mean number of returns to that state are \((1-f_{jj})^{-1}\) 
	\\Thus a state is transient when 
	\[ \sum\limits_{n-0}^{\infty}{p_{jj}^{(n)}} < \infty \]
	\subsection{Mean Recurrence Time}
	Mean recurrence time is the average number of steps needed to return to a recurrent step. 
	\[M_{jj} = \sum\limits_{n=1}^{\infty}{nf_{jj}^{(n)}}\]
	A \underline{finite} mean recurrence time makes a state \textit{positive-recurrent} or \textit{recurrent nonnull}  state
	\\An \underline{infinite} mean recurrence time makes a state \textit{null recurrent} - only occures when number of states is infinite.
	\subsubsection{No state is null recurrent in a finite Markov Chain}
	No state is null-recrrent and there must be at least on recurrent state in a finite markov chain. 
	\subsection{Mathmatical Analysis of Transitions}
	corresponding to \(f_{ii}^{(n)}\) let \(f_{ij}^{(n)}\) be defined.
	\[f_{ij}^{(n)} = p_{ij}\]
	\[  p_{ij}^{(n)} = \sum\limits_{l=1}^{n}{f_{ij}^{(l)}p_{ij}^{(n-l)}}, \quad n \geq 1.\]
	\[  f_{ij}^{(n)} = p_{ij}^{(n)} - \sum\limits_{l=1}^{n-1}{f_{ij}^{(l)}p_{ij}^{(n-l)}}}, \quad n \geq 1. \]
	Probability that state j is ever reached from i is 
	\[ f_{ij} = \sum\limits_{n=1}^{\infty}{f_{ij}^{(n)}}.\]
	If $f_{ij} < 1$ the process starting from i may never reach j. If $f_{ij} = 1$ the process must eventually hit j. 
\end{document}
